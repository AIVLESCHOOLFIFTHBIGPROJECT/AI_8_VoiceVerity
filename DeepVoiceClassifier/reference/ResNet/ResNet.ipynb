{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjC1eJr3QMyl"
      },
      "source": [
        "## Drive Mount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnXLUkbUQSdk",
        "outputId": "1e3b0eb5-e7b8-48f8-fd1b-26aea7f260fc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FH-2yTk5y5k"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-Tyf1sH-HOu",
        "outputId": "502a46b9-675e-4a01-8475-20fd97e68f3e"
      },
      "outputs": [],
      "source": [
        "#라이브러리 설치\n",
        "!pip install pydub\n",
        "!pip install onnxruntime\n",
        "!pip install spafe\n",
        "!pip install tf2onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XikpseHUYz19"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsJVEe8r-5o-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io.wavfile import read\n",
        "from spafe.features.lfcc import lfcc\n",
        "from spafe.features.cqcc import cqcc\n",
        "from spafe.utils.preprocessing import SlidingWindow\n",
        "from spafe.utils.vis import show_features\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tf2onnx\n",
        "from spafe.features.lfcc import linear_spectrogram,lfcc\n",
        "from spafe.utils.vis import show_spectrogram\n",
        "from spafe.utils.preprocessing import SlidingWindow\n",
        "from scipy.io.wavfile import read\n",
        "from spafe.utils.vis import show_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJzOLfsNcj9X"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgCwzLOX9viE"
      },
      "source": [
        "## window_shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KfaiZYXZDFjQ"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def split_wav_with_window_shift(file_path, file_name='test', window_size=5000, shift_size=1000, output_dir=\"output\", sr=16000):\n",
        "\n",
        "    if file_path[-3:] == \"wav\":\n",
        "      audio = AudioSegment.from_wav(file_path)\n",
        "\n",
        "    elif file_path[-3:] == \"mp3\":\n",
        "      audio = AudioSegment.from_mp3(file_path)\n",
        "\n",
        "    else:\n",
        "      return 'please check your input file'\n",
        "\n",
        "    # 출력 디렉토리 생성\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 오디오 길이 (밀리초 단위)\n",
        "    audio_length = len(audio)\n",
        "\n",
        "    # 오디오 길이가 5초보다 짧으면 반복하여 5초 이상으로 늘리기\n",
        "    if audio_length < window_size:\n",
        "        repeat_count = window_size // audio_length + 1\n",
        "        audio = audio * repeat_count\n",
        "        audio = audio[:window_size]  # 5초로 자르기\n",
        "        audio_length = len(audio)\n",
        "\n",
        "    # 오디오를 window_size 간격으로 나누고 shift_size 만큼 이동\n",
        "    for start in range(0, audio_length - window_size + 1, shift_size):\n",
        "        end = start + window_size\n",
        "        chunk = audio[start:end]\n",
        "\n",
        "        # 파일 저장\n",
        "        chunk_name = f\"{output_dir}/chunk_{file_name}_{start // 1000}_{end // 1000}.wav\"\n",
        "        chunk.export(chunk_name, format=\"wav\")\n",
        "        print(f\"Saved {chunk_name}\")\n",
        "\n",
        "    # 처음 1초에서 4초까지 부족한 부분을 반복으로 채우기\n",
        "    for i in range(1, 5):\n",
        "\n",
        "        chunk = audio[:i * 1000] * 5\n",
        "        chunk = chunk[:5000]\n",
        "\n",
        "        chunk_name = f\"{output_dir}/chunk_{file_name}_0_{i}.wav\"\n",
        "        chunk.export(chunk_name, format=\"wav\")\n",
        "        print(f\"Saved {chunk_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL9XJAhI6aNH"
      },
      "source": [
        "## make dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY2oWX_vQonF",
        "outputId": "296293de-781f-4381-8a91-01237922e504"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 파일 경로와 레이블을 저장할 리스트 초기화\n",
        "paths = []\n",
        "labels = []\n",
        "\n",
        "# 데이터셋이 저장된 루트 디렉토리 경로\n",
        "root_dir = '/content/drive/MyDrive/KT_BP/Data/DeepVoiceDataset'\n",
        "\n",
        "# 'Fake'라는 단어가 포함된 디렉토리 이름을 리스트로 저장\n",
        "lst = [file for file in os.listdir(root_dir) if 'Fake' in file]\n",
        "\n",
        "# 분할된 오디오 파일을 저장할 'Fake' 출력 디렉토리 경로\n",
        "output_dir = \"/content/Fake\"\n",
        "\n",
        "# 'Fake' 디렉토리 내의 각 서브디렉토리에 대해 작업 수행\n",
        "for subdir in lst:\n",
        "    subdir_path = os.path.join(root_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):  # 서브디렉토리가 존재하는지 확인\n",
        "        for filename in os.listdir(subdir_path):  # 서브디렉토리 내의 각 파일에 대해 작업 수행\n",
        "            file_path = os.path.join(subdir_path, filename)\n",
        "            # 오디오 파일을 일정한 크기로 분할하여 출력 디렉토리에 저장\n",
        "            split_wav_with_window_shift(\n",
        "                file_path,\n",
        "                window_size=5000,  # 윈도우 크기 (밀리초 단위)\n",
        "                shift_size=1000,   # 시프트 크기 (밀리초 단위)\n",
        "                output_dir=output_dir,\n",
        "                file_name=filename\n",
        "            )\n",
        "\n",
        "# 'Real'이라는 단어가 포함된 디렉토리 이름을 리스트로 저장\n",
        "lst = [file for file in os.listdir(root_dir) if 'Real' in file]\n",
        "\n",
        "# 분할된 오디오 파일을 저장할 'Real' 출력 디렉토리 경로\n",
        "output_dir = \"/content/Real\"\n",
        "\n",
        "# 'Real' 디렉토리 내의 각 서브디렉토리에 대해 작업 수행\n",
        "for subdir in lst:\n",
        "    subdir_path = os.path.join(root_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):  # 서브디렉토리가 존재하는지 확인\n",
        "        for filename in os.listdir(subdir_path):  # 서브디렉토리 내의 각 파일에 대해 작업 수행\n",
        "            file_path = os.path.join(subdir_path, filename)\n",
        "            # 오디오 파일을 일정한 크기로 분할하여 출력 디렉토리에 저장\n",
        "            split_wav_with_window_shift(\n",
        "                file_path,\n",
        "                window_size=5000,  # 윈도우 크기 (밀리초 단위)\n",
        "                shift_size=1000,   # 시프트 크기 (밀리초 단위)\n",
        "                output_dir=output_dir,\n",
        "                file_name=filename\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfBZbO2H-60W"
      },
      "source": [
        "## LFCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXQRK9Km9SNU",
        "outputId": "23f4329d-2f84-42b4-9e40-b89ac9600a81"
      },
      "outputs": [],
      "source": [
        "!pip install spafe #audio feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "pz51pcwDF3O7",
        "outputId": "f9a7502a-bc34-4db0-9b36-890ab1559f75"
      },
      "outputs": [],
      "source": [
        "from spafe.features.lfcc import linear_spectrogram\n",
        "from spafe.utils.vis import show_spectrogram\n",
        "from spafe.utils.preprocessing import SlidingWindow\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "# read audio\n",
        "fpath = \"/content/drive/MyDrive/KT_BP/Data/announcer_data/SPK003/SPK003YTNSO162F001.wav\"\n",
        "fs, sig = read(fpath)\n",
        "\n",
        "# compute spectrogram\n",
        "lSpec, lfreqs = linear_spectrogram(sig,\n",
        "                                   fs=fs,\n",
        "                                   pre_emph=0,\n",
        "                                   pre_emph_coeff=0.97,\n",
        "                                   window=SlidingWindow(0.03, 0.015, \"hamming\"),\n",
        "                                   nfilts=128,\n",
        "                                   nfft=2048,\n",
        "                                   low_freq=0,\n",
        "                                   high_freq=fs/2)\n",
        "\n",
        "# visualize spectrogram\n",
        "show_spectrogram(lSpec.T,\n",
        "                 fs,\n",
        "                 xmin=0,\n",
        "                 xmax=len(sig)/fs,\n",
        "                 ymin=0,\n",
        "                 ymax=(fs/2)/1000,\n",
        "                 dbf=80.0,\n",
        "                 xlabel=\"Time (s)\",\n",
        "                 ylabel=\"Frequency (kHz)\",\n",
        "                 title=\"Linear spectrogram (dB)\",\n",
        "                 cmap=\"jet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEFqWnlVb_f-",
        "outputId": "3b9aa8e6-ff56-475b-dca7-04f53dca3c46"
      },
      "outputs": [],
      "source": [
        "lSpec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "wD420NJx--Cn",
        "outputId": "71a38697-fde3-4d16-ac3f-63477acb9ef7"
      },
      "outputs": [],
      "source": [
        "from scipy.io.wavfile import read\n",
        "from spafe.features.lfcc import lfcc\n",
        "from spafe.utils.preprocessing import SlidingWindow\n",
        "from spafe.utils.vis import show_features\n",
        "\n",
        "# read audio\n",
        "#real audio\n",
        "fpath = \"/content/drive/MyDrive/KT_BP/Data/DeepVoiceDataset/Real_child/Aivle_Child_female_1520668291-3_53758.wav\"\n",
        "fs, sig = read(fpath)\n",
        "\n",
        "# compute lfccs\n",
        "lfccs  = lfcc(sig,\n",
        "              fs=fs,\n",
        "              pre_emph=1,\n",
        "              num_ceps=40,\n",
        "              pre_emph_coeff=0.97,\n",
        "              window=SlidingWindow(0.03, 0.015, \"hamming\"), #[hanning, bartlet, kaiser, blackman, hamming]\n",
        "              nfilts=40,\n",
        "              nfft=2048,\n",
        "              low_freq=0,\n",
        "              high_freq=8000,\n",
        "              normalize=\"mvn\")\n",
        "\n",
        "# visualize features\n",
        "show_features(lfccs, \"Linear Frequency Cepstral Coefﬁcients\", \"LFCC Index\", \"Frame Index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "8bZcxWnAJCB2",
        "outputId": "13465fe7-6c9b-48e9-ed5f-776a2459551b"
      },
      "outputs": [],
      "source": [
        "from scipy.io.wavfile import read\n",
        "from spafe.features.lfcc import lfcc\n",
        "from spafe.utils.preprocessing import SlidingWindow\n",
        "from spafe.utils.vis import show_features\n",
        "\n",
        "# read audio\n",
        "#real audio\n",
        "fpath = \"/content/drive/MyDrive/KT_BP/Data/DeepVoiceDataset/Fake_child/Aivle_Child_female_1520668291-3_53758_g.wav\"\n",
        "fs, sig = read(fpath)\n",
        "\n",
        "# compute lfccs\n",
        "lfccs  = lfcc(sig,\n",
        "              fs=fs,\n",
        "              pre_emph=1,\n",
        "              num_ceps=40,\n",
        "              pre_emph_coeff=0.97,\n",
        "              window=SlidingWindow(0.03, 0.015, \"hamming\"), #[hanning, bartlet, kaiser, blackman, hamming]\n",
        "              nfilts=40,\n",
        "              nfft=2048,\n",
        "              low_freq=0,\n",
        "              high_freq=8000,\n",
        "              normalize=\"mvn\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# visualize features\n",
        "show_features(-lfccs, \"Linear Frequency Cepstral Coefﬁcients\", \"LFCC Index\", \"Frame Index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6LNtSlqD_Bs",
        "outputId": "62dc3060-3bee-4a17-ff5e-8765f0bc248a"
      },
      "outputs": [],
      "source": [
        "lfccs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_T1Ea_HEkTM"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "audio, _ = librosa.load(fpath, sr=16000)\n",
        "# Extract features (example: using Mel-Frequency Cepstral Coefficients)\n",
        "mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMdlOhiXEv--",
        "outputId": "7528b8f0-9432-438d-9bff-6c78119154d4"
      },
      "outputs": [],
      "source": [
        "mfccs.shape, mfccs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7NuPdb1RhQ"
      },
      "source": [
        "## CQCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "N8R-1bn61ROO",
        "outputId": "29e8ea6f-5f97-4bb8-e876-749485342624"
      },
      "outputs": [],
      "source": [
        "from scipy.io.wavfile import read\n",
        "from spafe.features.cqcc import cqcc\n",
        "from spafe.utils.preprocessing import SlidingWindow\n",
        "from spafe.utils.vis import show_features\n",
        "\n",
        "# read audio\n",
        "fpath = \"/content/drive/MyDrive/KT_BP/Data/DeepVoiceDataset/Real_child/Aivle_Child_female_1522218484-0_18368.wav\"\n",
        "fs, sig = read(fpath)\n",
        "\n",
        "# 설정된 프레임 수를 맞추기 위해 필요한 신호 길이 계산\n",
        "desired_frames = 128\n",
        "frame_length = int(0.03 * fs)  # 0.03초 윈도우 길이\n",
        "frame_shift = int(0.0155 * fs)  # 0.0155초 프레임 간 시간차\n",
        "signal_length = desired_frames * frame_shift + frame_length - frame_shift\n",
        "\n",
        "# 신호 길이를 조정\n",
        "if len(sig) < signal_length:\n",
        "    sig = np.pad(sig, (0, signal_length - len(sig)), 'constant')\n",
        "else:\n",
        "    sig = sig[:signal_length]\n",
        "\n",
        "\n",
        "# compute cqccs\n",
        "cqccs  = cqcc(sig,\n",
        "              fs=fs,\n",
        "              num_ceps=128,\n",
        "              pre_emph=1,\n",
        "              pre_emph_coeff=0.97,\n",
        "              window=SlidingWindow(0.03, 0.0155, \"hamming\"),\n",
        "              nfft=2048,\n",
        "              low_freq=0,\n",
        "              high_freq=fs/2,\n",
        "              normalize=\"mvn\")\n",
        "\n",
        "# visualize features\n",
        "show_features(cqccs, \"Constant Q-Transform Cepstral Coefﬁcients\", \"CQCC Index\", \"Frame Index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqlRW7Sw1Z83",
        "outputId": "76cef86e-0a32-4439-d1fd-a3dd95d9e9e2"
      },
      "outputs": [],
      "source": [
        "cqccs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9PnHjX6--fi"
      },
      "source": [
        "## CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG6_Yv8c3wZf",
        "outputId": "57d14e46-5efd-49f3-9657-f5a4214c5fe0"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "paths = []\n",
        "labels = []\n",
        "root_dir = '/content/'\n",
        "lst = [file for file in os.listdir(root_dir) if 'Fake' in file or 'Real' in file]\n",
        "\n",
        "for subdir in lst:\n",
        "    subdir_path = os.path.join(root_dir, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        for filename in os.listdir(subdir_path):\n",
        "            file_path = os.path.join(subdir_path, filename)\n",
        "            paths.append(file_path)\n",
        "            labels.append(subdir[:4])\n",
        "\n",
        "print('Dataset is loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuBi7x4TQf1j",
        "outputId": "32efc41c-4b37-449a-bcb4-6fa2127669dd"
      },
      "outputs": [],
      "source": [
        "lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h6FAV9j5HWY"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df['speech'] = paths\n",
        "df['label'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUzI8kLs5Rxg",
        "outputId": "1d52abec-52a4-4fe2-e0b4-63be6cae6927"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJXTmZfmAWel"
      },
      "source": [
        "## extract features (train_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56rWkt4lVjSE"
      },
      "outputs": [],
      "source": [
        "def extract_features(audio_path, max_length=500, model='mfcc', n_fil=40):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # 'Fake' 또는 'Real'이 파일 이름에 포함된 디렉토리만 선택\n",
        "    lst = [file for file in os.listdir(audio_path) if 'Fake' in file or 'Real' in file]\n",
        "\n",
        "    for folder in lst:\n",
        "        print(folder)\n",
        "        folder_path = os.path.join(audio_path, folder)\n",
        "\n",
        "        for file in tqdm(os.listdir(folder_path)):\n",
        "            file_path = os.path.join(folder_path, file)\n",
        "\n",
        "            try:\n",
        "                if model == 'mfcc':\n",
        "                    # 오디오 파일 로드\n",
        "                    audio, _ = librosa.load(file_path, sr=16000)\n",
        "                    # MFCC 특징 추출\n",
        "                    mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=n_fil)\n",
        "\n",
        "                    # 특징 배열을 고정 길이로 패딩 또는 자르기\n",
        "                    if mfccs.shape[1] < max_length:\n",
        "                        mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        mfccs = mfccs[:, :max_length]\n",
        "\n",
        "                    features.append(mfccs)\n",
        "\n",
        "                elif model == 'lfcc':\n",
        "                    fs, sig = read(file_path)\n",
        "                    # LFCC 특징 추출\n",
        "                    lfccs = lfcc(sig, fs=fs, pre_emph=1, num_ceps=n_fil, pre_emph_coeff=0.97,\n",
        "                                 window=SlidingWindow(0.03, 0.015, \"hamming\"),\n",
        "                                 nfilts=n_fil,\n",
        "                                 nfft=2048,\n",
        "                                 low_freq=0,\n",
        "                                 high_freq=8000,\n",
        "                                 normalize=\"mvn\")\n",
        "                    lfccs = lfccs.reshape(n_fil, -1)\n",
        "\n",
        "                    if lfccs.shape[1] < max_length:\n",
        "                        lfccs = np.pad(lfccs, ((0, 0), (0, max_length - lfccs.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        lfccs = lfccs[:, :max_length]\n",
        "\n",
        "                    features.append(lfccs)\n",
        "\n",
        "                elif model == 'mel':\n",
        "                    # 오디오 파일 로드\n",
        "                    audio, _ = librosa.load(file_path, sr=16000)\n",
        "                    # Mel 스펙트로그램 추출\n",
        "                    mel_spec = librosa.feature.melspectrogram(y=audio, sr=16000, n_mels=n_fil, hop_length=512)\n",
        "                    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "                    if mel_spec_db.shape[1] < max_length:\n",
        "                        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, max_length - mel_spec_db.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        mel_spec_db = mel_spec_db[:, :max_length]\n",
        "\n",
        "                    features.append(mel_spec_db)\n",
        "\n",
        "                elif model == 'linear':\n",
        "                    fs, sig = read(file_path)\n",
        "                    # 선형 스펙트로그램 추출\n",
        "                    lSpec, lfreqs = linear_spectrogram(sig,\n",
        "                                                       fs=fs,\n",
        "                                                       pre_emph=0,\n",
        "                                                       pre_emph_coeff=0.97,\n",
        "                                                       window=SlidingWindow(0.03, 0.015, \"hamming\"),\n",
        "                                                       nfilts=n_fil,\n",
        "                                                       nfft=2048,\n",
        "                                                       low_freq=0,\n",
        "                                                       high_freq=fs / 2)\n",
        "                    lSpecs = lSpec.reshape(n_fil, -1)\n",
        "\n",
        "                    if lSpecs.shape[1] < max_length:\n",
        "                        lSpecs = np.pad(lSpecs, ((0, 0), (0, max_length - lSpecs.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        lSpecs = lSpecs[:, :max_length]\n",
        "\n",
        "                    features.append(lSpecs)\n",
        "\n",
        "                # 레이블 할당 ('Fake'는 1, 'Real'은 0)\n",
        "                if folder[:4] == 'Fake':\n",
        "                    labels.append(1)  # 1 for fake\n",
        "                else:\n",
        "                    labels.append(0)  # 0 for real\n",
        "\n",
        "            except Exception as e:\n",
        "                # 오류 발생 시 파일 경로와 오류 메시지 출력\n",
        "                print(f\"Error encountered while parsing file: {file_path} {e}\")\n",
        "                continue\n",
        "\n",
        "    # 특징과 레이블 배열 반환\n",
        "    return np.array(features), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbuMI7bD5SqZ",
        "outputId": "3b321f0a-6857-4dd2-d961-41cc19d57320"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "audio_path = '/content/'\n",
        "x, y = extract_features(audio_path, model='linear', n_fil=128)\n",
        "\n",
        "print(\"Features shape:\", x.shape)\n",
        "print(\"Labels shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKRc6yFZAdpm"
      },
      "source": [
        "## data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPjcW5_p5al-"
      },
      "outputs": [],
      "source": [
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = .2, random_state=202405, stratify=y)\n",
        "xtrain,xval,ytrain,yval = train_test_split(xtrain,ytrain,test_size = .2, random_state=202405, stratify=ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqzAbkqK-Je5",
        "outputId": "8afb1958-7158-44d1-c2d2-4de8e012f174"
      },
      "outputs": [],
      "source": [
        "xtrain.shape, xtest.shape, xval.shape, yval.shape, ytrain.shape, ytest.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn1ajgFAcsn-"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpgKTwkpB79x"
      },
      "source": [
        "## ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHUX4wZ_9uyZ",
        "outputId": "2973000c-5c06-4386-e6ed-133d0edc8715"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import ResNet50\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Flatten, Conv2D, Input, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.backend import clear_session\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "clear_session()\n",
        "\n",
        "# ResNet 모델 정의\n",
        "input_shape = (128, 500, 1)  # MFCC 형상에 맞춤\n",
        "inputs = Input(shape=input_shape)\n",
        "base_model = ResNet50(weights=None, include_top=False, input_tensor=inputs)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "x_train = xtrain[..., np.newaxis]\n",
        "x_val = xval[..., np.newaxis]\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x_train, ytrain, epochs=40, batch_size=64, validation_data=(x_val, yval), callbacks=[es,])\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(x_val, yval)\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "\n",
        "# 검증 손실 및 정확도 그래프 그리기\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 검증 손실 그래프\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Validation and Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 검증 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.title('Validation and Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df2I-1T1bf3E"
      },
      "outputs": [],
      "source": [
        "x_test = xtest[..., np.newaxis]\n",
        "ypred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA7Tjh2GZgCe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "ypred_binary = np.where(ypred > 0.5, 1, 0)\n",
        "report = classification_report(ypred_binary, ytest)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duJwK-RLJHkN"
      },
      "source": [
        "## ResNet152V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv7YWOakDhke"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.applications import ResNet152V2\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Flatten, Conv2D, Input, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.backend import clear_session\n",
        "\n",
        "clear_session()\n",
        "\n",
        "# ResNet 모델 정의\n",
        "input_shape = (128, 500, 1)  # MFCC 형상에 맞춤\n",
        "inputs = Input(shape=input_shape)\n",
        "base_model = ResNet152V2(weights=None, include_top=False, input_tensor=inputs)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x_train, ytrain, epochs=50, batch_size=64, validation_data=(x_test, ytest), callbacks=[es,])\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, ytest)\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "\n",
        "# 검증 손실 및 정확도 그래프 그리기\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 검증 손실 그래프\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('ResNet152V2 Validation and Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 검증 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.title('ResNet152V2 Validation and Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp4jHrFgI25d"
      },
      "outputs": [],
      "source": [
        "x_test = xtest[..., np.newaxis]\n",
        "ypred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O1Ulq7kWdab"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "ypred_binary = np.where(ypred > 0.5, 1, 0)\n",
        "report = classification_report(ypred_binary, ytest)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enp8naHRJq6o"
      },
      "source": [
        "## EfficientNetB4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-NXNRmgJrx8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.applications import EfficientNetB7\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Flatten, Conv2D, Input, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.backend import clear_session\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "clear_session()\n",
        "\n",
        "# EfficientNetB4 모델 정의\n",
        "input_shape = (128, 500, 1)  # MFCC 형상에 맞춤\n",
        "inputs = Input(shape=input_shape)\n",
        "base_model = EfficientNetB7(weights=None, include_top=False, input_tensor=inputs)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x_train, ytrain, epochs=50, batch_size=64, validation_data=(x_test, ytest), callbacks=[es,])\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, ytest)\n",
        "print(f'Test loss: {loss}')\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "\n",
        "# 검증 손실 및 정확도 그래프 그리기\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 검증 손실 그래프\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('EfficientNetB4 Validation and Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 검증 정확도 그래프\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.title('EfficientNetB4 Validation and Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtX2vt_sXYS7"
      },
      "outputs": [],
      "source": [
        "x_test = xtest[..., np.newaxis]\n",
        "ypred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfoSm7qcXZx6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "ypred_binary = np.where(ypred > 0.5, 1, 0)\n",
        "report = classification_report(ypred_binary, ytest)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWRHNQlp9Z4U"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qsQJp2Gs3RxJ",
        "outputId": "cb3d3b76-0f4a-4e60-c3e6-20a50a6ec910"
      },
      "outputs": [],
      "source": [
        "# Function to save the model to ONNX\n",
        "def save_model_to_onnx(model, input_shape, output_path, opset_version=13):\n",
        "    spec = (tf.TensorSpec((None, *input_shape), tf.float32, name=\"input\"),)\n",
        "    output_model_path = f\"{output_path}.onnx\"\n",
        "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=opset_version)\n",
        "    with open(output_model_path, \"wb\") as f:\n",
        "        f.write(model_proto.SerializeToString())\n",
        "    print(f\"Model saved to {output_model_path}\")\n",
        "\n",
        "# Save the trained model to ONNX\n",
        "output_path = \"/content/drive/MyDrive/Fake_voice_detection_model/window_shift_5_linear_model\"\n",
        "save_model_to_onnx(model, (128, 500, 1), output_path, opset_version=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXvpumfadEqb"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MojSTjz1V5Vx"
      },
      "source": [
        "## extract features (test_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi6KrGHpWinZ"
      },
      "outputs": [],
      "source": [
        "def extract_test_features(audio_path, file_name, max_length=500, model='mfcc', n_fil=40, label=False):\n",
        "    features = []  # 특징 벡터를 저장할 리스트\n",
        "    labels = []  # 레이블을 저장할 리스트\n",
        "    lst = [file for file in os.listdir(audio_path) if file_name in file]  # 지정된 파일 이름을 포함하는 파일 목록 생성\n",
        "    for folder in lst:\n",
        "        print(folder)  # 현재 처리 중인 폴더 출력\n",
        "        folder_path = os.path.join(audio_path, folder)  # 폴더 경로 생성\n",
        "        for file in tqdm(os.listdir(folder_path)):  # 폴더 내 파일들에 대해 반복\n",
        "            file_path = os.path.join(folder_path, file)  # 파일 경로 생성\n",
        "            try:\n",
        "                if model == 'mfcc':\n",
        "                    # 오디오 파일 로드\n",
        "                    audio, _ = librosa.load(file_path, sr=16000)\n",
        "                    # MFCC 특징 추출\n",
        "                    mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=n_fil)\n",
        "                    # 특징 벡터를 고정된 길이로 패딩 또는 자르기\n",
        "                    if mfccs.shape[1] < max_length:\n",
        "                        mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        mfccs = mfccs[:, :max_length]\n",
        "                    features.append(mfccs)\n",
        "\n",
        "                elif model == 'lfcc':\n",
        "                    # 오디오 파일 로드\n",
        "                    fs, sig = read(file_path)\n",
        "                    # LFCC 특징 추출\n",
        "                    lfccs = lfcc(sig, fs=fs, pre_emph=1, num_ceps=n_fil, pre_emph_coeff=0.97,\n",
        "                                 window=SlidingWindow(0.03, 0.015, \"hamming\"),\n",
        "                                 nfilts=n_fil,\n",
        "                                 nfft=2048,\n",
        "                                 low_freq=0,\n",
        "                                 high_freq=8000,\n",
        "                                 normalize=\"mvn\")\n",
        "                    lfccs = lfccs.reshape(n_fil, -1)\n",
        "                    # 특징 벡터를 고정된 길이로 패딩 또는 자르기\n",
        "                    if lfccs.shape[1] < max_length:\n",
        "                        lfccs = np.pad(lfccs, ((0, 0), (0, max_length - lfccs.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        lfccs = lfccs[:, :max_length]\n",
        "                    features.append(lfccs)\n",
        "\n",
        "                elif model == 'linear':\n",
        "                    # 오디오 파일 로드\n",
        "                    fs, sig = read(file_path)\n",
        "                    # Linear Spectrogram 특징 추출\n",
        "                    lSpec, lfreqs = linear_spectrogram(sig,\n",
        "                                                       fs=fs,\n",
        "                                                       pre_emph=0,\n",
        "                                                       pre_emph_coeff=0.97,\n",
        "                                                       window=SlidingWindow(0.03, 0.015, \"hamming\"),\n",
        "                                                       nfilts=n_fil,\n",
        "                                                       nfft=2048,\n",
        "                                                       low_freq=0,\n",
        "                                                       high_freq=fs/2)\n",
        "                    lSpecs = lSpec.reshape(n_fil, -1)\n",
        "                    # 특징 벡터를 고정된 길이로 패딩 또는 자르기\n",
        "                    if lSpecs.shape[1] < max_length:\n",
        "                        lSpecs = np.pad(lSpecs, ((0, 0), (0, max_length - lSpecs.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        lSpecs = lSpecs[:, :max_length]\n",
        "                    features.append(lSpecs)\n",
        "\n",
        "                elif model == 'mel':\n",
        "                    # 오디오 파일 로드\n",
        "                    audio, _ = librosa.load(file_path, sr=16000)\n",
        "                    # Mel Spectrogram 특징 추출\n",
        "                    mel_spec = librosa.feature.melspectrogram(y=audio, sr=16000, n_mels=n_fil, hop_length=512)\n",
        "                    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "                    # 특징 벡터를 고정된 길이로 패딩 또는 자르기\n",
        "                    if mel_spec_db.shape[1] < max_length:\n",
        "                        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, max_length - mel_spec_db.shape[1])), mode='constant')\n",
        "                    else:\n",
        "                        mel_spec_db = mel_spec_db[:, :max_length]\n",
        "                    features.append(mel_spec_db)\n",
        "\n",
        "                if not label:\n",
        "                    labels.append(1)  # label이 False인 경우 1 (fake)를 레이블로 추가\n",
        "                else:\n",
        "                    labels.append(0)  # label이 True인 경우 0 (real)을 레이블로 추가\n",
        "            except Exception as e:\n",
        "                print(f\"Error encountered while parsing file: {file_path} {e}\")  # 파일 처리 중 에러 발생 시 출력\n",
        "                continue\n",
        "    return np.array(features), np.array(labels)  # 특징 벡터와 레이블 배열 반환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5oRnGQv9dCl"
      },
      "source": [
        "##Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC6QtQndCJjM"
      },
      "outputs": [],
      "source": [
        "def saved_model_pred(model_path, x_test, y_test):\n",
        "    # ONNX 모델을 로드하여 추론 세션을 생성합니다.\n",
        "    sess = ort.InferenceSession(model_path)\n",
        "\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "\n",
        "    # x_test의 마지막 차원에 새로운 축을 추가합니다.\n",
        "    x_test = x_test[..., np.newaxis]\n",
        "\n",
        "    input_data = np.array(x_test, dtype=np.float32)\n",
        "\n",
        "    outputs = sess.run(None, {input_name: input_data})\n",
        "\n",
        "    data = np.array(outputs, dtype=np.float32)\n",
        "\n",
        "    # 데이터가 0.5보다 큰 경우 1로, 그렇지 않은 경우 0으로 변환하여 이진 데이터를 생성합니다.\n",
        "    binary_data = (data > 0.5).astype(int)\n",
        "\n",
        "    return data, binary_data.sum() / len(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI7DtVkvrrCw"
      },
      "source": [
        "## model test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXGE9DGv6Wdh",
        "outputId": "57171386-02dd-4098-f44c-83a9835fda8f"
      },
      "outputs": [],
      "source": [
        "# # 파일 경로와 출력을 원하는 디렉토리 설정\n",
        "file_path = '/content/tmp.mp3'\n",
        "output_dir = \"api_fake_2\"\n",
        "\n",
        "# 5초(5000밀리초) window 크기와 1초(1000밀리초) shift 크기로 파일 나누기\n",
        "split_wav_with_window_shift(file_path, window_size=5000, shift_size=1000, output_dir=output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXpeFipWskjc",
        "outputId": "df8c37f3-0bb7-4a11-9e80-eaeedce92a7d"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "audio_path = '/content/'\n",
        "output_dir = \"nj\"\n",
        "x, y = extract_test_features(audio_path, output_dir ,max_length=500, model='mel', n_fil=128)\n",
        "\n",
        "print(\"Features shape:\", x.shape)\n",
        "print(\"Labels shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV-zDkULL8Ug"
      },
      "outputs": [],
      "source": [
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tnCf-tuGGZn",
        "outputId": "14034a84-2163-45cb-f383-b81f0300529f"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/Fake_voice_detection_model/window_shift_5_mel_model.onnx\"\n",
        "x_pred, pred = saved_model_pred(model_path, x, y)\n",
        "\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODjTP6BMzJLH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jfBZbO2H-60W",
        "jX7NuPdb1RhQ",
        "V9PnHjX6--fi"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
