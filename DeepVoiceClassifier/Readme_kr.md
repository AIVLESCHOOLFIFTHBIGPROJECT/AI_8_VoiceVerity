# 딥 보이스 분류기 (Deep Voice Classifier)

## 우리 프로젝트

### 프로젝트 개요

- 딥 러닝과 오디오 합성 기술의 확산은 매우 현실적인 인공 음성, 즉 딥 페이크 음성의 생성을 가능하게 한다. 이러한 합성 음성은 실제 인간의 음성을 매우 가깝게 모방할 수 있어 보안, 인증, 미디어 무결성 등 다양한 분야에서 큰 도전 과제를 제시한다. 따라서 진짜 인간 음성과 딥 페이크 음성을 효과적으로 구별하는 것은 매우 중요하다.

### 프로젝트 목표

- 본 프로젝트의 주요 목표는 딥 페이크 음성을 감지할 수 있는 정확하고 신뢰할 수 있는 분류기를 개발하는 것이다. 이를 위해 우리는 고급 머신 러닝 기술 모델을 활용할 것이다. 오디오 신호를 멜-스펙트로그램 이미지로 변환하여 정교한 시각적 분류 방법을 적용함으로써 진짜 음성과 가짜 음성을 식별하고 구별할 수 있다.

### 프로젝트 주요 단계

1. 라이브러리 호출:

    - 오디오 처리 및 모델 훈련에 필요한 numpy, pandas, torch, torchaudio, matplotlib, tensorflow 등의 라이브러리를 호출한다.

2. WAV 파일 로드 및 처리:

    - torchaudio를 사용하여 오디오 데이터를 로드하고 일관성과 전처리를 보장한다.

3. 오디오 분할 및 노이즈 제거:

    - 오디오 분리 모델을 사용하여 배경 노이즈로부터 인간의 음성을 분리하여 오디오 데이터의 명확성을 높인다.

4. 오디오를 1초 간격으로 분할:

    - 정밀한 분석과 특성 추출을 위해 정리된 오디오 데이터를 1초 간격으로 분할한다.

5. 분할된 오디오를 멜-스펙트로그램으로 변환:

    - 각 1초 오디오 세그먼트를 torchaudio를 사용하여 멜-스펙트로그램으로 변환한다.

6. 비전 트랜스포머(ViT) 설정 및 훈련:

    - 멜-스펙트로그램을 입력 데이터로 사용하여 FastViT 모델을 준비하고 훈련시킨다.

7. 모델 평가:

    - 검증 또는 테스트 데이터를 사용하여 훈련된 모델의 성능을 평가한다.

8. ONNX 형식으로 모델 배포:

    - 훈련된 모델을 ONNX 형식으로 변환하여 배포한다.

### 추론

1. WAV 또는 MP3 음성 데이터 로드:

    - 분류하려는 오디오 데이터를 torchaudio를 사용하여 로드한다. 이 단계는 오디오 데이터가 추가 처리를 위해 적합한 형식인지 확인한다.

2. 오디오 데이터를 멜-스펙트로그램 이미지로 변환:

    - 필요한 경우 오디오를 분할한 후 멜-스펙트로그램 변환을 적용하여 오디오 데이터를 멜-스펙트로그램 이미지로 변환한다.

3. 멜-스펙트로그램 이미지를 모델에 입력:

    - 멜-스펙트로그램 이미지를 훈련된 ViT 모델에 입력한다. 멜-스펙트로그램은 ViT 모델의 입력 요구 사항에 맞게 크기를 조정하고 정규화할 필요가 있다.

4. 클래스 예측:

    - 모델을 사용하여 각 멜-스펙트로그램 세그먼트의 클래스를 예측한다. 모델은 음성이 딥 페이크인지 실제 인간 음성인지 나타내는 확률 또는 클래스를 출력한다.

### DeepVoiceClassifier 흐름 소개

#### 요약

- 본 프로젝트의 목적은 Vision Transformer(ViT) 모델을 사용하여 딥 페이크 음성을 감지하는 것이다. 오디오 데이터를 멜-스펙트로그램 이미지로 변환함으로써 ViT 모델의 기능을 활용하여 실제 인간 음성과 딥 페이크 음성을 구별할 수 있다. 이 접근 방식은 딥 페이크 음성 감지의 정확성과 효율성을 향상시키기 위해 설계되었다.

#### 소개

- 딥 페이크 기술의 확산으로 인공적으로 생성된 음성을 감지하는 것이 중요해졌다. 본 프로젝트는 속도와 성능이 최적화된 비전 트랜스포머 모델을 활용하여 이 문제를 해결하고자 한다. 오디오 신호를 멜-스펙트로그램 이미지로 변환함으로써 오디오 도메인에 시각적 분류 기술을 효과적으로 적용할 수 있다.

#### 관련 연구

- 이전 연구들은 전통적인 머신 러닝 알고리즘, 컨볼루션 신경망(CNN), 순환 신경망(RNN) 등 다양한 딥 페이크 음성 감지 방법을 탐구했다. 본 연구는 이미지 분류 작업에서 효율적인 아키텍처와 높은 정확도를 자랑하는 ViT의 고급 기능을 활용함으로써 이러한 기초 위에 구축되었다.

#### DeepVoiceClassifier

우리 접근 방식의 핵심 단계는 다음과 같다:

1. 특성 추출:

    - 오디오 신호를 1초 간격으로 분할하고 멜-스펙트로그램 이미지로 변환한다.

2. 모델 아키텍처:

    - 멜-스펙트로그램 이미지를 처리하기 위해 ViT 모델을 사용한다.

3. 훈련:

    - 실제 인간 음성과 딥 페이크 음성으로 구성된 라벨이 있는 데이터셋으로 모델을 훈련시킨다.

4. 추론:

    - 훈련된 모델을 사용하여 새로운 오디오 샘플을 분류하고, 그것이 진짜인지 가짜인지에 대한 예측을 제공한다.

#### 실험

- 우리의 접근 방식을 검증하기 위해 라벨이 있는 오디오 샘플 데이터셋을 사용하여 실험을 수행했다. 데이터셋에는 최신 합성 기술을 사용하여 생성된 진짜 인간 음성 및 딥 페이크 음성이 포함되어 있다. 우리는 데이터셋을 훈련 및 테스트 세트로 나누고, ViT 모델을 훈련시키고, 정확도, 정밀도, 재현율, F1-점수와 같은 표준 메트릭을 사용하여 성능을 평가했다.

  - 1분 음성 기준 정확도

        - ViT : 98.8% ACC

        - FastViT : 98.8% ACC

        - Shallow CNN : 96.2% ACC

        - ResNet : 95.8% ACC

    - 1분 음성 기준 추론 속도

      - ViT : 5s

        - FastViT : 8s

        - Shallow CNN : 11s

        - ResNet : 8s

#### 결론

- 우리의 실험은 멜-스펙트로그램 이미지에 적용된 ViT 모델이 딥 페이크 음성을 감지하는 데 있어 높은 정확도를 달성한다는 것을 보여준다. 이 접근 방식은 실시간 딥 페이크 음성 감지에 강력한 솔루션을 제공하며, 보안, 인증 및 미디어 검증에 잠재적으로 응용될 수 있다. 향후 작업은 모델의 추가 최적화 및 감지 기능을 향상시키기 위한 추가 기능 탐색을 포함할 수 있다. 고급 트랜스포머 모델인 Vision Transformer 모델을 활용함으로써 우리는 딥 페이크 음성 감지 분야에서 새로운 표준을 설정하는 것을 목표로 한다.
